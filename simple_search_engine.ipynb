{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do:\n",
    "- OK mudar estrutura de set para lista ordenada\n",
    "- OK implementar and/or\n",
    "- usar dataset imdb\n",
    "- implementar tf-df\n",
    "- implementar filtros\n",
    "- implementar sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    {\n",
    "        'id': 1,\n",
    "        'title': 'Star Wars',\n",
    "    },\n",
    "    {\n",
    "        'id': 2,\n",
    "        'title': 'Star Wars IV',\n",
    "    },\n",
    "    {\n",
    "        'id': 3,\n",
    "        'title': 'Star Wars V',\n",
    "    },\n",
    "    {\n",
    "        'id': 4,\n",
    "        'title': 'Star Trek',\n",
    "    },\n",
    "    {\n",
    "        'id': 5,\n",
    "        'title': 'Star Wars: the Clone Wars',\n",
    "    },\n",
    "    {\n",
    "        'id': 6,\n",
    "        'title': 'Toy Story',\n",
    "    },\n",
    "    {\n",
    "        'id': 7,\n",
    "        'title': 'Solo: a Star Wars story',\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/leonardo.wajnsztok/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/leonardo.wajnsztok/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['star', 'wars', 'clone', 'wars']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import unidecode\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "  \n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def analyse(text):\n",
    "    text = ''.join([t for t in text if t not in string.punctuation])\n",
    "    text = text.lower().strip()\n",
    "    text = unidecode.unidecode(text)\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    return tokens\n",
    "\n",
    "analyse('Star Wars: the Clone Wars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sortedcontainers import SortedKeyList\n",
    "import numpy\n",
    "\n",
    "\n",
    "class TokenInfo:\n",
    "    def __init__(self, term, doc_id):\n",
    "        self.term = term\n",
    "        self.doc_id = doc_id\n",
    "        self.tf = 1\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'({self.doc_id}, tf: {self.tf})'\n",
    "    \n",
    "class Index:\n",
    "    def __init__(self, documents=None):\n",
    "        self.index = defaultdict(lambda: SortedKeyList(key=lambda x: x.doc_id))\n",
    "        self.df = defaultdict(int)\n",
    "        self.documents = {}\n",
    "        if documents:\n",
    "            self.build(documents)\n",
    "\n",
    "        \n",
    "    def __repr__(self):\n",
    "        rep = ['term freq:']\n",
    "        for k, v in self.index.items():\n",
    "            rep.append(f'{k}: {v}')\n",
    "        rep.append('\\n')\n",
    "        rep.append(f'document freq:\\n{dict(self.df)}')\n",
    "        return '\\n'.join(rep)\n",
    "        \n",
    "    def build(self, documents):\n",
    "        for doc in documents:\n",
    "            self.add_document(doc)\n",
    "            \n",
    "    def add_document(self, doc):\n",
    "        text = doc['title']\n",
    "        doc_id = doc['id']\n",
    "        self.documents[doc_id] = doc\n",
    "        \n",
    "        tokens = analyse(text)\n",
    "        tokens_set = set()\n",
    "        \n",
    "        for t in tokens:\n",
    "            if t not in tokens_set:\n",
    "                tokens_set.add(t)\n",
    "                self.df[t] += 1\n",
    "                \n",
    "            term_docs = self.index[t]\n",
    "            term_info_idx = self._find_doc_in_list(term_docs, doc_id)\n",
    "            if term_info_idx:\n",
    "                term_docs[term_info_idx].tf += 1\n",
    "            else:\n",
    "                term_docs.add(TokenInfo(t, doc_id))\n",
    "                \n",
    "    def _find_doc_in_list(self, elements, value):\n",
    "        index = elements.bisect_key_left(value)\n",
    "        if index < len(elements) and elements[index].doc_id == value:\n",
    "            return index\n",
    "                \n",
    "    def _and(self, docs1, docs2):\n",
    "        intersection = []\n",
    "        i, j = 0,0\n",
    "        while i < len(docs1) and j < len(docs2):\n",
    "            if docs1[i] == docs2[j]:\n",
    "                intersection.append(docs1[i])\n",
    "                i += 1\n",
    "                j += 1\n",
    "            elif docs1[i] > docs2[j]:\n",
    "                j += 1\n",
    "            else:\n",
    "                i += 1\n",
    "        return intersection\n",
    "    \n",
    "    def _or(self, docs1, docs2):\n",
    "        union = []\n",
    "        i, j = 0,0\n",
    "        while i < len(docs1) and j < len(docs2):\n",
    "            if docs1[i] == docs2[j] :\n",
    "                union.append(docs1[i])\n",
    "                i += 1\n",
    "                j += 1\n",
    "            elif docs1[i] > docs2[j]:\n",
    "                union.append(docs2[j])\n",
    "                j += 1\n",
    "            else:\n",
    "                union.append(docs1[i])\n",
    "                i += 1\n",
    "        \n",
    "        while i < len(docs1):\n",
    "            union.append(docs1[i])\n",
    "            i += 1\n",
    "\n",
    "        while j < len(docs2):\n",
    "            union.append(docs2[j])\n",
    "            j += 1\n",
    "            \n",
    "        return union\n",
    "    \n",
    "    def _get_docs(self, docs):\n",
    "        return [self.documents[d] for d in docs]\n",
    "                \n",
    "    def search(self, text, operation='AND'):\n",
    "        tokens = analyse(text)\n",
    "        found_docs = []\n",
    "        \n",
    "        for t in tokens:\n",
    "            if t in self.index:\n",
    "                new_docs = [d.doc_id for d in self.index[t]]\n",
    "                \n",
    "                if found_docs:\n",
    "                    if operation == 'AND':\n",
    "                        found_docs = self._and(found_docs, new_docs)\n",
    "                    elif operation == 'OR':\n",
    "                        found_docs = self._or(found_docs, new_docs)\n",
    "                    else:\n",
    "                        raise NotImplementedError(f'Operation \"{operation}\" not implemented!')\n",
    "                else:\n",
    "                    found_docs.extend(new_docs)\n",
    "\n",
    "        return self._get_docs(found_docs)\n",
    "    \n",
    "\n",
    "index = Index(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.search('toy wars', operation='AND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 6, 'title': 'Toy Story'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.search('toy', operation='AND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1, 'title': 'Star Wars'},\n",
       " {'id': 2, 'title': 'Star Wars IV'},\n",
       " {'id': 3, 'title': 'Star Wars V'},\n",
       " {'id': 5, 'title': 'Star Wars: the Clone Wars'},\n",
       " {'id': 6, 'title': 'Toy Story'},\n",
       " {'id': 7, 'title': 'Solo: a Star Wars story'}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.search('toy wars', operation='OR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
